train:
  seed: 3407
  epochs: 1
  log_interval: 10
  val_interval: 100
  save_dir: runs/ckpts
  # dummy_loss: none   # ignored when LLM CE loss is used

dataset:
  data_dir: /workspace/data/data_lc/sslt/csl_dental
  # Optional augmentations used by dataset/my_dataset
  augmentations: "speed"  # "speed", "mask", 
  aug_prob: 0.5
  aug_speed_min: 0.9
  aug_speed_max: 1.1
  aug_mask_prob: 0.05

data:
  batch_size: 8
  num_workers: 4
  # If dataset.data_dir is not provided, fallback dummy will be used:
  T: 32
  train_length: 256
  val_length: 128
  # custom_builder: your_module:your_builder  # returns (train_loader, val_loader, nclass)

llm:
  model_name_or_path: /workspace/data/data_lc/sslt/Qwen2.5-0.5B
  trust_remote_code: true
  max_text_len: 128
  num_prefix_tokens: 1
  adapter_hidden: 512
  freeze_lm: false
  gradient_checkpointing: false
  bot_token: "<BOT>"

streaming:
  enabled: true
  window: 16
  stride: 8
  drop_last: true
  loss_reduction: mean

nclass: 10
