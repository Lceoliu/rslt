train:
  seed: 3407
  epochs: 15
  log_logits: true
  log_interval: 500
  val_interval: 250
  save_dir_root: runs
  save_every: 1200
  resume_from: "/workspace/rslt/runs/stage1/20251001_153225/checkpoints" # modify this
  ckpt_tag: "global_step20000" # specify the checkpoint tag to load, e.g. "global_step1000", or leave it as an empty string to load the latest checkpoint
  resume_for_new_stage: true # if true, will load model weights but not optimizer, lr_scheduler states
  visual_lr: 1e-5
  llm_lr: 1e-6

dataset:
  data_dir: /2023234331/data/shm/format_data/csl_shm
  seed: 3407
  augmentations: ""
  aug_prob: 0.0
  aug_speed_min: 0.9
  aug_speed_max: 1.1
  aug_mask_prob: 0.05
  pad_last: true
  min_reserved_ratio: 0.6
  window: 32
  stride: 16

data:
  batch_size: 8
  num_workers: 4
  conf_threshold: 0.1
  T: 32
  train_length: 256
  val_length: 128

model:
  parts: ["body", "face", "left_hand", "right_hand", "fullbody"]
  drop_conf: true
  part_embed_dim: 256
  tokens_per_chunk: 10
  uni_gcn:
    proj_dim: 256
    temporal_kernel: 5
    adaptive: true
    dropout: 0.0
  chunk_transformer:
    layers: 3
    heads: 8
    dropout: 0.1
    mlp_dim: 512

llm:
  model_name_or_path: /workspace/Qwen
  trust_remote_code: true
  max_text_len: 128
  freeze_lm: false
  gradient_checkpointing: false
  boc_token: "<BOC>"
  eoc_token: "<EOC>"
  bot_token: "<BOT>"
  eot_token: "<EOT>"
  contrastive_tau: 0.07
  contrastive_neg_k: 64
  contrastive_alpha: 0.0

decoding:
  max_new_tokens: 64
  do_sample: true
  temperature: 0.3
  top_k: 10

nclass: 10
