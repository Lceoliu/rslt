train:
  seed: 3407
  epochs: 10 
  log_logits: false
  log_interval: 100000
  val_interval: 100000
  save_dir_root: runs
  save_every: 100000
  resume_from: '/workspace/rslt/runs/20251013_113232/checkpoints' # modify this
  ckpt_tag: "best" # specify the checkpoint tag to load, e.g. "global_step1000", or leave it as an empty string to load the latest checkpoint
  visual_lr: 1e-5 # 略微降低学习率，稳定训练
  llm_lr: 5e-7 

dataset:
  data_dirs: /2023234331/data/csl_news
  have_sub_dirs: true
  seed: 3407
  augmentations: 'speed'
  aug_prob: 0.3
  aug_speed_min: 0.8
  aug_speed_max: 1.2
  aug_mask_prob: 0.15
  pad_last: true
  min_reserved_ratio: 0.8
  window: 45
  stride: 30

data:
  batch_size: 8
  num_workers: 4
  conf_threshold: 0.1
  T: 32
  train_length: 256
  val_length: 128

model:
  parts:
  - body
  - face
  - left_hand
  - right_hand
  - fullbody
  drop_conf: true
  part_embed_dim: 256
  tokens_per_chunk: 12
  uni_gcn:
    proj_dim: 256
    temporal_kernel: 7
    adaptive: true
    dropout: 0.1 # 略微增加GCN的dropout
  chunk_transformer:
    layers: 5
    heads: 8
    dropout: 0.2 # 增加Transformer的dropout
    mlp_dim: 512

llm:
  model_name_or_path: /workspace/Qwen
  trust_remote_code: true
  max_text_len: 128
  freeze_lm: false # 核心：此阶段训练整个模型
  gradient_checkpointing: false
  boc_token: <BOC>
  eoc_token: <EOC>
  bot_token: <BOT>
  eot_token: <EOT>
  contrastive_tau: 0.08
  contrastive_neg_k: 256
  contrastive_projection_dim: 256 # Dimension of the contrastive projection head
  # --- 关键改动：同时使用对比损失和多样性损失 ---
  contrastive_alpha: 0.1 # TODO: 调整对比损失权重
  diversity_alpha: 0.1   # TODO: 调整多样性损失权重
  compute_special_token_loss: false # TODO: 是否计算特殊token的损失

decoding:
  max_new_tokens: 64
  do_sample: true
  temperature: 0.3
  top_k: 15
